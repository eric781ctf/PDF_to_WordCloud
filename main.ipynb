{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "under-shipping",
   "metadata": {},
   "source": [
    "# Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organized-combining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import jieba\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, ImageColorGenerator, STOPWORDS\n",
    "from scipy.ndimage import gaussian_gradient_magnitude\n",
    "import imageio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-general",
   "metadata": {},
   "source": [
    "# Read PDF file and transform to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-radar",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_file = './test.pdf'\n",
    "PDF = pdfplumber.open(PDF_file)\n",
    "print('No. of all the pages: ',len(PDF.pages))\n",
    "\n",
    "count = 0\n",
    "text=''\n",
    "# choose the range of the pages you need\n",
    "for x in range(354,388):\n",
    "    page = PDF.pages[x]\n",
    "    text += page.extract_text()\n",
    "    count += 1\n",
    "print('The pages we needto transform to text: ',count)\n",
    "print('len of text: ', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-rates",
   "metadata": {},
   "source": [
    "# Use jieba to cut the text and change to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = jieba.lcut(text, cut_all=False)\n",
    "print(type(text_list))\n",
    "print(len(text_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-warehouse",
   "metadata": {},
   "source": [
    "# Remove the words we don't need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "french-charity",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list=[]\n",
    "def remove_stop_words(file_name,seg_list):\n",
    "    with open(file_name,'r',encoding=\"utf-8\") as f:\n",
    "        stop_words = f.readlines()\n",
    "    stop_words = [stop_word.rstrip() for stop_word in stop_words]\n",
    "    for seg in seg_list:\n",
    "        if seg not in stop_words:\n",
    "            if seg != '\\n' and seg != ' ' and seg != 'on' and seg != 'te' and seg != 'le' and seg != 'en' and seg != 'Co' and seg != ',':\n",
    "                new_list.append(seg)\n",
    "    return new_list\n",
    "#The words you want to remove is based on the words in this document\n",
    "file_name = './test.txt'\n",
    "seg_list = remove_stop_words(file_name,text_list)\n",
    "print(len(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-rover",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#統計詞頻\n",
    "def count_segment_freq(seg_list):\n",
    "    seg_df = pd.DataFrame(seg_list,columns=['text'])\n",
    "    seg_df['count'] = 1\n",
    "    sef_freq = seg_df.groupby('text')['count'].sum().sort_values(ascending=False)\n",
    "    sef_freq = pd.DataFrame(sef_freq)\n",
    "    return sef_freq\n",
    "\n",
    "sef_freq = count_segment_freq(seg_list)\n",
    "sef_freq.reset_index(inplace = True)\n",
    "#print the 50 most popular words\n",
    "print(sef_freq.head(50))\n",
    "\n",
    "#The file where you want to write the result\n",
    "file_path = './count.xlsx'\n",
    "writer = pd.ExcelWriter(file_path)\n",
    "sef_freq.to_excel(writer, columns=['text','count'], index=False,encoding='utf-8',sheet_name='Sheet')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_list=' '.join(seg_list)\n",
    "# Mask image\n",
    "mask_color = np.array(Image.open('./test.jpg'))\n",
    "mask_color = mask_color[::3, ::3]\n",
    "mask_image = mask_color.copy()\n",
    "mask_image[mask_image.sum(axis=2) == 0] = 255\n",
    "\n",
    "# Edge detection\n",
    "edges = np.mean([gaussian_gradient_magnitude(mask_color[:, :, i]/255., 2) for i in range(3)], axis=0)\n",
    "mask_image[edges > .08] = 255\n",
    "\n",
    "#change the pic\n",
    "#mask=mask_image,\n",
    "wc = WordCloud(max_words=500,\n",
    "               font_path='C:/Windows/Fonts/kaiu.ttf',\n",
    "               max_font_size=65,\n",
    "               min_font_size=10,\n",
    "               scale=20,\n",
    "               background_color='white',\n",
    "               prefer_horizontal=0.9,\n",
    "               random_state=None,\n",
    "               relative_scaling=1)\n",
    "\n",
    "wc.generate(wc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hispanic-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.axis('off')\n",
    "plt.imshow(wc)\n",
    "plt.show()\n",
    "wc.to_file(\"C:/Users/eric/Desktop/wc.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "declared-library",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-nature",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
